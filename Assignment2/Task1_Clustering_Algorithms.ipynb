{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3P-1sJ4yiqR"
      },
      "source": [
        "# EE214 Assignment 2: Task 1 - Clustering Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9UpzjeSS0yN"
      },
      "source": [
        "This notebook implements a clustering pipeline on the UCI Dermatology dataset (~358 samples, 34 features) using unsupervised‑learning techniques. It covers data loading, preprocessing (scaling and PCA), clustering (K‑Means and DBSCAN), and evaluation using Elbow method, with visualizations after the clustering step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIa7bQIpcDOB",
        "outputId": "a6b51ad0-3c13-4fd9-d84e-32db21eadb8f"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo #This installs the UCIML library to import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kgxAdpQncz4G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "Load the complete UCI Dermatology dataset (358 patient records, 34 numeric attributes). Only the feature matrix X_train is used for clustering; the original diagnosis labels are kept aside for optional reference or plotting but are not used in any algorithm training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag2hNqzUbrJ0",
        "outputId": "029c09fd-d331-4a69-afda-88dbb676298b"
      },
      "outputs": [],
      "source": [
        "dataset = fetch_ucirepo(id=33) # Dermatolegy Dataset with 6 classes\n",
        "\n",
        "\n",
        "# Access the data\n",
        "X = dataset.data.features\n",
        "y = dataset.data.targets\n",
        "\n",
        "# Example usage (replace with your desired processing):\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, T_train, T_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check for and remove NaN values\n",
        "X_train = X_train.dropna()\n",
        "T_train = T_train.loc[X_train.index]\n",
        "\n",
        "X_test = X_test.dropna()\n",
        "T_test = T_test.loc[X_test.index]  # Keep target values aligned with features\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", T_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", T_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "_3hLvETPcXZ8",
        "outputId": "7c3c3b0b-14ae-482b-bf34-522d0b78bfbb"
      },
      "outputs": [],
      "source": [
        "X_train #Dataset information and samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "imU-pIKUFv4F",
        "outputId": "a2fd28c9-32bd-4b23-c2fe-718a9d220510"
      },
      "outputs": [],
      "source": [
        "T_train #See class information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Apply PCA to reduce dimensionality and scale the data using Standard scalers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "scaling"
      },
      "outputs": [],
      "source": [
        "# Standard Scaling\n",
        "# Scale X_train to zero mean and unit variance using StandardScaler\n",
        "standard_scaler = StandardScaler()\n",
        "X_train_s = standard_scaler.fit_transform(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "PpkvODyxdVVm",
        "outputId": "aa9b1efc-8d59-4930-c81e-6bbef36ed1f6"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=5, random_state=42) # You can adjust n_components as needed\n",
        "X_train_pca = pca.fit_transform(X_train_s)\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_.sum())\n",
        "# Verify PCA output shape\n",
        "print('PCA output shape:', X_train_pca.shape)\n",
        "\n",
        "# Visualization\n",
        "color_num = 3\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "xs = X_train_pca[:,0]\n",
        "ys = X_train_pca[:,1]\n",
        "\n",
        "# Assuming T_train is a DataFrame and 'class' is the column with labels\n",
        "label_encoder = LabelEncoder()\n",
        "numerical_labels = label_encoder.fit_transform(T_train['class'])\n",
        "\n",
        "scatter = plt.scatter(xs, ys, c=numerical_labels, cmap='tab10')\n",
        "\n",
        "legend1 = plt.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Labels\")\n",
        "plt.gca().add_artist(legend1) # Add the legend to the plot\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_functions"
      },
      "source": [
        "## Visualization Functions\n",
        "\n",
        "Define functions for visualizing clustering results (image plots, t-SNE, PCA scatter plots)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tsne"
      },
      "outputs": [],
      "source": [
        "# t-SNE Visualization (for reference)\n",
        "def vec_vis(x, y, T, n, title): #Note that we make two plots here where the later original plot using actual labels.\n",
        "    plt.rcParams['figure.figsize'] = [20, 8]\n",
        "    color_num = max(n, 5)\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "    xs = x[:, 0]\n",
        "    ys = x[:, 1]\n",
        "    ax1.set_title('t-SNE Visualization with Clustering')\n",
        "    scatter = ax1.scatter(xs, ys, c=y, cmap=plt.get_cmap('rainbow', color_num))\n",
        "    legend = ax1.legend(*scatter.legend_elements(), loc='upper right', title='Clusters')\n",
        "    ax2.set_title('t-SNE Visualization with True Labels')\n",
        "    scatter = ax2.scatter(xs, ys, c=T, cmap=plt.get_cmap('rainbow', color_num))\n",
        "    legend = ax2.legend(*scatter.legend_elements(), loc='upper right', title='Labels')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pca_plot"
      },
      "outputs": [],
      "source": [
        "# PCA Scatter Plot\n",
        "def pca_scatter_plot(X_pca, Y, T, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=Y, cmap='rainbow', alpha=0.5)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.colorbar(label='Cluster')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wHSbJp1HAyk"
      },
      "source": [
        "define and fit model for t-SNE visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tsne_compute"
      },
      "outputs": [],
      "source": [
        "# Compute t-SNE for visualization (takes 2–3 minutes)\n",
        "model = TSNE(learning_rate=300, random_state=3)\n",
        "TSNE_X = model.fit_transform(X_train_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clustering"
      },
      "source": [
        "## Clustering- Implementing KMeans algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K7hIrT0uJ-Lx"
      },
      "outputs": [],
      "source": [
        "class KMeans:\n",
        "\n",
        "  def __init__(self, n_clusters=1, random_state=500):\n",
        "      '''\n",
        "      parameter:\n",
        "      n_clusters: desired number of clusters\n",
        "      random_state: random state for initializing the centroids\n",
        "      '''\n",
        "      assert n_clusters >= 1\n",
        "      self.n_clusters = n_clusters\n",
        "      self.random_state = random_state\n",
        "      self.cluster_centers_ = None    # cluster centers (=centriods) will be initialized in 'fit' method\n",
        "      self.inertia_ = None            # SSE, set in 'fit' method\n",
        "\n",
        "  def fit(self, X):\n",
        "      '''\n",
        "      parameter:\n",
        "      X: data\n",
        "      shape of X: (number of sample, feature dimensions)\n",
        "\n",
        "      Compute the centroids that can achieve the minimum variance within clusters given a (training) data X.\n",
        "      Clustering is performed iteratively until the centroids are not changed, i.e., until the optimum is reached.\n",
        "      The centroids obtained here; \"self.cluster_centers_\" will be used in \"predict\" method.\n",
        "\n",
        "      It returns a array of cluster labels for data X.\n",
        "      For each sample in X, the cluster label means the index of the nearest center among centers in self.cluster_centers_. Thus, the cluster labels range from 0 to (n_clusters-1).\n",
        "\n",
        "      Return:\n",
        "      cluster_labels: numpy array of cluster labels for all data samples in X\n",
        "      '''\n",
        "\n",
        "      # convert to ndarray in case a DataFrame is passed\n",
        "      X = np.asarray(X, dtype=float)\n",
        "\n",
        "      # Here, the centroids is initialized with arbitrary samples of X.\n",
        "      np.random.seed(self.random_state)  # the choice of these random samples is governed by the \"self.random_state\".\n",
        "      initial_cluster_centers_idx = np.random.choice(len(X), self.n_clusters, replace=False) # choose the 'n_clusters' number of samples in X to take them as initial centriods\n",
        "      self.cluster_centers_ = X[initial_cluster_centers_idx]  # initial centroids\n",
        "\n",
        "\n",
        "      # fill in the blank --------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # --------------------------------------------------------\n",
        "\n",
        "      # compute inertia (sum of squared distances to closest centroid)\n",
        "      sq_dists = np.sum((X - self.cluster_centers_[cluster_labels])**2, axis=1)\n",
        "      self.inertia_ = sq_dists.sum()\n",
        "\n",
        "      return cluster_labels\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "      '''\n",
        "      parameter:\n",
        "      X: data\n",
        "      shape of X: (number of sample, feature dimensions)\n",
        "\n",
        "      For each sample in the data X (might be testset), predict the cluster using the centroids obtained from \"fit\" method.\n",
        "      It returns an array of cluster labels for data X.\n",
        "\n",
        "      Return:\n",
        "      cluster_labels: numpy array of cluster labels for all data samples in X (testset)\n",
        "      '''\n",
        "      assert self.cluster_centers_ is not None\n",
        "\n",
        "      # fill in the blank --------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "      # --------------------------------------------------------\n",
        "\n",
        "      return cluster_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eigHGJGiSViv"
      },
      "source": [
        "Apply K-Means for n clusters, followed by visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "kmeans",
        "outputId": "647ff13e-8080-4165-b560-df5836663dc2"
      },
      "outputs": [],
      "source": [
        "# K-Means Clustering\n",
        "k = 6 # number of clusters\n",
        "\n",
        "# Cluster X_train_pca into n clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "Y_train_kmeans = kmeans.fit(X_train_pca)\n",
        "\n",
        "# K-Means Visualizations\n",
        "#img_plt(X_train, Y_train_kmeans, 'K-Means Clustering')\n",
        "vec_vis(TSNE_X, Y_train_kmeans, numerical_labels, 6, 'K-Means t-SNE Visualization')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## Determining number of Clusters\n",
        "\n",
        " Determining the number of clusters is a critical step in unsupervised learning, as it directly impacts the quality of clustering results. For K-Means, the Elbow method and Silhouette score are commonly used, where the Elbow method identifies the point where adding more clusters yields diminishing reductions in within-cluster sum-of-squares (inertia), and the Silhouette score measures cluster cohesion and separation. For Agglomerative Clustering, similar metrics like the Silhouette score can be applied across a range of cluster numbers, or dendrograms can be analyzed to identify natural cluster splits, though computational constraints may limit dendrogram use for large datasets. For DBSCAN, which does not require a predefined number of clusters, the optimal `eps` and `min_samples` can be tuned by evaluating the number of clusters and noise points produced, often using a k-distance plot to estimate a suitable `eps` value based on the distance to the k-th nearest neighbor.\n",
        "\n",
        "Evaluate K-Means clustering using the Elbow method for k=2 to 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "i9bt4djQMcPj",
        "outputId": "97a59353-a96a-4804-c1fb-54fa6d423796"
      },
      "outputs": [],
      "source": [
        "# K-Means: Elbow Method\n",
        "# Compute inertia for k=2 to 10\n",
        "inertia_list = []\n",
        "\n",
        "k_range = range(2, 10)\n",
        "for k in k_range:\n",
        "    model = KMeans(n_clusters=k, random_state=1)\n",
        "    labels = model.fit(X_train_pca)\n",
        "    inertia_list.append(model.inertia_)\n",
        "\n",
        "# Verify lists length\n",
        "print('Inertia list length:', len(inertia_list))\n",
        "\n",
        "# Plot Elbow\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(k_range, inertia_list, \"bo-\")\n",
        "plt.title(\"Elbow Method for K‑Means (PCA space)\")\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEQXkzQ4PFTD"
      },
      "source": [
        "## Clustering- Implementing DBSCAN algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0YtkFgZ9Jokt"
      },
      "outputs": [],
      "source": [
        "class DBSCAN:\n",
        "\n",
        "    def __init__(self, eps, min_samples):\n",
        "        self.eps = eps\n",
        "        self.min_samples = min_samples\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        '''\n",
        "        parameter:\n",
        "        X: data\n",
        "        shape of X: (number of sample, feature dimensions)\n",
        "\n",
        "        It returns a array of cluster labels for each sample point.\n",
        "        Here, the cluster labels are numbered starting from 1.\n",
        "        Moreover, \"-1\" means noise.\n",
        "        '''\n",
        "        self.X = X\n",
        "        self.cluster_labels = np.zeros((len(X)))              # Initialize all the cluster labels as 0\n",
        "        self.current_label = 0\n",
        "        self.labeled_indices = set()                          # for tracking all visited(already labeled) points\n",
        "        self.core_sample_indices_ = []                        # for tracking all core samples (for future visualization)\n",
        "\n",
        "        # declare something if you think you need ------------\n",
        "       \n",
        "        # ----------------------------------------------------\n",
        "\n",
        "        while np.any(self.cluster_labels == 0):               # repeat until there's no point that has not been visited any more\n",
        "            p_idx = self.pick_arbitrary_point()               # pick arbitrary point among points that are not visited so far (fill the blanks for \"pick_arbitrary_point\" below)\n",
        "            if self.is_core_sample(p_idx):                    # check if it is core sample (fill the blanks for \"is_core_sample\" below)\n",
        "                self.current_label += 1                       # define new cluster label for current visiting\n",
        "                self.visit_all_successive_neighbors(p_idx)    # visit all the neighbors and label them in succession (fill the blanks for \"visit_all_successive_neighbors\" below)\n",
        "            else:\n",
        "                self.cluster_labels[p_idx] = -1               # label it as noise if it is not a core sample in the first place\n",
        "\n",
        "        return self.cluster_labels\n",
        "\n",
        "    def pick_arbitrary_point(self):\n",
        "        '''\n",
        "        Pick arbitrary point among points that are not visited so far (for next successive visiting).\n",
        "        It returns an \"index\" of point(\"p_idx\"), not a data point itself.\n",
        "        '''\n",
        "        # fill in the blank --------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        assert self.cluster_labels[p_idx] == 0                # for sanity check\n",
        "        return p_idx\n",
        "\n",
        "    def is_core_sample(self, p_idx):\n",
        "        '''\n",
        "        parameter:\n",
        "        p_idx: index of point\n",
        "\n",
        "        Check whether the \"p_idx\" is a core sample or not.\n",
        "        If it is, return True. Otherwise, return False.\n",
        "        You can use \"get_neighbors\" method, which is defined below.\n",
        "        You can define a core sample that has greater than or equal to min_samples points in its neighbor, where the point itself is also included in its neighbor.\n",
        "        '''\n",
        "        # fill in the blank --------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "\n",
        "    def visit_all_successive_neighbors(self, p_idx):\n",
        "        '''\n",
        "        parameter:\n",
        "        p_idx: index of point\n",
        "\n",
        "        Visit all the neighbors of \"p_idx\" as well as the neighbors of all the visited points if they are the core points themselves.\n",
        "        Assign current cluster label everytime you visited. But you don't need to relabel them if they are already allocated to a specific cluster.\n",
        "        It returns nothing but modifies \"self.cluster_labels\" in-place when labeling.\n",
        "        '''\n",
        "        all_neighbors_indices = {p_idx}\n",
        "\n",
        "        while all_neighbors_indices:\n",
        "\n",
        "        # fill in the blank --------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "\n",
        "    def get_neighbors(self, p_idx):\n",
        "        '''\n",
        "        parameter:\n",
        "        p_idx: index of point\n",
        "\n",
        "        It returns a \"set of indices\" of neighbors of \"p_idx\" point.\n",
        "        l2 norm will be considered for computing distances.\n",
        "        '''\n",
        "        # fill in the blank --------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "        # --------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "iZA5xGCMPPE4",
        "outputId": "14b3ae4f-b123-4598-cbec-b9dcb712b673"
      },
      "outputs": [],
      "source": [
        "# DBScan Clustering\n",
        "\n",
        "# Cluster X_train_pca using DBSCAN\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=1) #dummy values for eps and min_samples\n",
        "Y_train_dbscan = dbscan.fit_predict(X_train_pca)\n",
        "\n",
        "# DBscan Visualizations\n",
        "#img_plt(X_train, Y_train_kmeans, 'K-Means Clustering (k=10)')\n",
        "vec_vis(TSNE_X, Y_train_dbscan, numerical_labels, 6, 'DBScan t-SNE Visualization')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
